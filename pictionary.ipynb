{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje para la clasificación de dibujos en Pictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, vamos a simular el proceso de aprendizaje del software del juego Pictionary con una red neuronal que utilizará una capa LSTM para las secuencias de trazos y luego una capa densa para determinar la categoría a la que pertenece un dibujo en concreto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, cargaremos en un vector las distintas categorías posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = open('categories.txt','r').read().splitlines()\n",
    "np.random.shuffle(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una cantidad concreta de categorías para usar en el aprendizaje y un máximo de dibujos para cada categoría.\n",
    "A continuación, iremos agregando los dibujos correspondientes a los conjuntos de entrenamiento, validación y test. En cuanto a las categorías, serán almacenadas como números enteros desde 0 hasta max_cat-1 en formato one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría hat , agregados  1  de  8\n",
      "Categoría jail , agregados  2  de  8\n",
      "Categoría door , agregados  3  de  8\n",
      "Categoría drill , agregados  4  de  8\n",
      "Categoría traffic light , agregados  5  de  8\n",
      "Categoría teddy-bear , agregados  6  de  8\n",
      "Categoría tent , agregados  7  de  8\n",
      "Categoría mountain , agregados  8  de  8\n"
     ]
    }
   ],
   "source": [
    "max_cat = 8\n",
    "max_pic = 500\n",
    "x_train = []\n",
    "x_val = []\n",
    "x_test = []\n",
    "trainY = []\n",
    "valY = []\n",
    "testY = []\n",
    "i = 1\n",
    "for c in categories[:max_cat]:\n",
    "    npz_file = np.load('all_sketches/' + c + '.npz', encoding='latin1', allow_pickle=True)\n",
    "    train_set = np.array(npz_file['train'])[:max_pic]\n",
    "    val_set = np.array(npz_file['valid'])[:max_pic]\n",
    "    test_set = np.array(npz_file['test'])[:max_pic]\n",
    "    x_train.append(train_set)\n",
    "    x_val.append(val_set)\n",
    "    x_test.append(test_set)\n",
    "    for k in range(0,max_pic):\n",
    "        trainY.append([i-1])\n",
    "        valY.append([i-1])\n",
    "        testY.append([i-1])\n",
    "    print(\"Categoría\", c, \", agregados \", i, \" de \", max_cat)\n",
    "    i = i + 1\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "x_val = np.array(x_val)\n",
    "x_test = np.array(x_test)\n",
    "trainY = np.array(trainY)\n",
    "valY = np.array(valY)\n",
    "testY = np.array(testY)\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(trainY)\n",
    "\n",
    "trainY = one_hot_encoder.transform(trainY)\n",
    "valY = one_hot_encoder.transform(valY)\n",
    "testY = one_hot_encoder.transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso será determinar cuál es la cantidad máxima de trazos posibles entre todos los dibujos obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "max_size = 0\n",
    "for i in range(0,max_cat):\n",
    "    for j in range(0,x_train.shape[1]):\n",
    "        if (x_train[i][j].shape[0] > max_size):\n",
    "            max_size = x_train[i][j].shape[0]\n",
    "for i in range(0,max_cat):\n",
    "    for j in range(0,x_val.shape[1]):\n",
    "        if (x_val[i][j].shape[0] > max_size):\n",
    "            max_size = x_val[i][j].shape[0]\n",
    "for i in range(0,max_cat):\n",
    "    for j in range(0,x_test.shape[1]):\n",
    "        if (x_test[i][j].shape[0] > max_size):\n",
    "            max_size = x_test[i][j].shape[0]\n",
    "print(max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, redimensionamos los dibujos de los tres conjuntos de datos de forma que todos tienen el mmismo número de trazos (max_size), rellenando con ceros los trazos añadidos, indicando que no se desplazaría más el lápiz sobre el lienzo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.zeros((max_cat*x_train.shape[1],max_size,3))\n",
    "\n",
    "for i in range(0,max_cat):\n",
    "    for j in range(0,x_train.shape[1]):\n",
    "        current_size = x_train[i][j].shape[0]\n",
    "        for k in range(0,max_size):\n",
    "            if (k < current_size):\n",
    "                trainX[x_train.shape[1]*i+j][k] = x_train[i][j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX = np.zeros((max_cat*x_val.shape[1],max_size,3))\n",
    "\n",
    "for i in range(0,max_cat):\n",
    "    for j in range(0,x_val.shape[1]):\n",
    "        current_size = x_val[i][j].shape[0]\n",
    "        for k in range(0,max_size):\n",
    "            if (k < current_size):\n",
    "                valX[x_val.shape[1]*i+j][k] = x_val[i][j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.zeros((max_cat*x_test.shape[1],max_size,3))\n",
    "\n",
    "for i in range(0,max_cat):\n",
    "    for j in range(0,x_test.shape[1]):\n",
    "        current_size = x_test[i][j].shape[0]\n",
    "        for k in range(0,max_size):\n",
    "            if (k < current_size):\n",
    "                testX[x_test.shape[1]*i+j][k] = x_test[i][j][k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que los datos han sido tratados, procedemos a entrenar nuestra red neuronal. El número de épocas que estableceremos será de 70 con un tamaño de batch igual a 4. Para el aprendizaje, utilizaremos el optimizador ADAM y la función de pérdida será el error cuadrático medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples\n",
      "Epoch 1/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.1034\n",
      "Epoch 2/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0928\n",
      "Epoch 3/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0861\n",
      "Epoch 4/70\n",
      "4000/4000 [==============================] - 86s 21ms/sample - loss: 0.0817\n",
      "Epoch 5/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0684\n",
      "Epoch 6/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0589\n",
      "Epoch 7/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0528\n",
      "Epoch 8/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0471\n",
      "Epoch 9/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0417\n",
      "Epoch 10/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0368\n",
      "Epoch 11/70\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.0330\n",
      "Epoch 12/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0326\n",
      "Epoch 13/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0272\n",
      "Epoch 14/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0248\n",
      "Epoch 15/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0231\n",
      "Epoch 16/70\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.0226\n",
      "Epoch 17/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0198\n",
      "Epoch 18/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0216\n",
      "Epoch 19/70\n",
      "4000/4000 [==============================] - 90s 22ms/sample - loss: 0.0195\n",
      "Epoch 20/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0170\n",
      "Epoch 21/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0153\n",
      "Epoch 22/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0161\n",
      "Epoch 23/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0151\n",
      "Epoch 24/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0152\n",
      "Epoch 25/70\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.0140\n",
      "Epoch 26/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0131\n",
      "Epoch 27/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0132\n",
      "Epoch 28/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0121\n",
      "Epoch 29/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0130\n",
      "Epoch 30/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0121\n",
      "Epoch 31/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0102\n",
      "Epoch 32/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0113\n",
      "Epoch 33/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0110\n",
      "Epoch 34/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0103\n",
      "Epoch 35/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0126\n",
      "Epoch 36/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0113\n",
      "Epoch 37/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0098\n",
      "Epoch 38/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0117s - lo\n",
      "Epoch 39/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0103\n",
      "Epoch 40/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0090\n",
      "Epoch 41/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0112\n",
      "Epoch 42/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0093\n",
      "Epoch 43/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0101\n",
      "Epoch 44/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0088\n",
      "Epoch 45/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0073\n",
      "Epoch 46/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0090\n",
      "Epoch 47/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0088\n",
      "Epoch 48/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0094\n",
      "Epoch 49/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0095\n",
      "Epoch 50/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0085\n",
      "Epoch 51/70\n",
      "4000/4000 [==============================] - 90s 22ms/sample - loss: 0.0088\n",
      "Epoch 52/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0094\n",
      "Epoch 53/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0094\n",
      "Epoch 54/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0076\n",
      "Epoch 55/70\n",
      "4000/4000 [==============================] - 90s 22ms/sample - loss: 0.0085\n",
      "Epoch 56/70\n",
      "4000/4000 [==============================] - 90s 22ms/sample - loss: 0.0094\n",
      "Epoch 57/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0072\n",
      "Epoch 58/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0092\n",
      "Epoch 59/70\n",
      "4000/4000 [==============================] - 89s 22ms/sample - loss: 0.0095\n",
      "Epoch 60/70\n",
      "4000/4000 [==============================] - 91s 23ms/sample - loss: 0.0079\n",
      "Epoch 61/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0077\n",
      "Epoch 62/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0098\n",
      "Epoch 63/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0100\n",
      "Epoch 64/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0077\n",
      "Epoch 65/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0067\n",
      "Epoch 66/70\n",
      "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.0085\n",
      "Epoch 67/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0081\n",
      "Epoch 68/70\n",
      "4000/4000 [==============================] - 88s 22ms/sample - loss: 0.0071\n",
      "Epoch 69/70\n",
      "4000/4000 [==============================] - 90s 22ms/sample - loss: 0.0070\n",
      "Epoch 70/70\n",
      "4000/4000 [==============================] - 90s 23ms/sample - loss: 0.0065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c7d17b9348>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(max_size))\n",
    "model.add(Dense(max_cat))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=70, batch_size=4, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el entrenamiento de la red, procedemos a predecir, para el conjunto de test, las categorías a las que pertenecen los dibujos del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01026595e+00,  3.02074552e-02,  5.25811948e-02, ...,\n",
       "        -4.50789742e-03,  3.77699733e-03, -2.11828277e-02],\n",
       "       [ 1.04170942e+00,  4.80854511e-03, -1.29290335e-02, ...,\n",
       "         1.48105063e-03, -1.51795894e-03, -1.08353347e-02],\n",
       "       [ 1.02358580e+00,  9.19699669e-03, -1.80695727e-02, ...,\n",
       "        -1.83173828e-03,  3.00072134e-04, -8.39058310e-03],\n",
       "       ...,\n",
       "       [-1.42145660e-02, -2.26341188e-03,  2.09575817e-02, ...,\n",
       "         4.85654734e-03,  9.48014855e-03,  9.96936262e-01],\n",
       "       [-1.47412177e-02, -1.73132867e-03,  2.07811221e-02, ...,\n",
       "         4.80932556e-03,  8.50234181e-03,  9.99132752e-01],\n",
       "       [-1.47200543e-02, -1.73144042e-03,  2.07746252e-02, ...,\n",
       "         4.80678491e-03,  8.49179178e-03,  9.99131322e-01]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos determinar cuál es el error cometido en cada categoría, contando para cada una el númmero de errores en función del número de dibujos de cada categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la categoría hat :  11.799999999999999  %\n",
      "Error en la categoría jail :  6.0  %\n",
      "Error en la categoría door :  11.4  %\n",
      "Error en la categoría drill :  25.6  %\n",
      "Error en la categoría traffic light :  7.6  %\n",
      "Error en la categoría teddy-bear :  2.6  %\n",
      "Error en la categoría tent :  17.4  %\n",
      "Error en la categoría mountain :  6.4  %\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,max_cat):\n",
    "    total_error = 0\n",
    "    for j in range(i*max_pic, (i+1)*max_pic):\n",
    "        if (np.argmax(testPredict[j]) != np.argmax(testY[j])): total_error += 1\n",
    "    print(\"Error en la categoría\", categories[i], \": \", (total_error/(len(testY)/max_cat))*100, \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, la red comete más errores al intentar predecir etiquetas de dibujos como taladros, meintras que otros más sencillos de dibujar como las montañas son más fáciles de predecir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
